# proxy_toolkit.py
import random
import requests
from flask import Flask, request
from bs4 import BeautifulSoup
import threading
import http.server
import socketserver
import urllib.request

app = Flask(__name__)
proxy_list = []

#########################
# 1. Web Proxy
#########################
@app.route('/')
def home():
    return '''
        <form action="/proxy">
            URL to visit: <input name="url" type="text" />
            <input type="submit" value="Go" />
        </form>
    '''

@app.route('/proxy')
def proxy():
    target_url = request.args.get('url')
    if not target_url:
        return 'URL is required.'
    try:
        resp = requests.get(target_url)
        return resp.text
    except Exception as e:
        return f"Error: {str(e)}"

#########################
# 2. Basic Local Proxy Server
#########################
class LocalProxy(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        try:
            url = self.path[1:]
            with urllib.request.urlopen(url) as response:
                self.send_response(200)
                self.end_headers()
                self.wfile.write(response.read())
        except Exception as e:
            self.send_error(500, str(e))

def run_local_proxy():
    port = 8888
    with socketserver.TCPServer(("", port), LocalProxy) as httpd:
        print(f"[LocalProxy] Running on port {port}")
        httpd.serve_forever()

#########################
# 3. Proxy Scraper
#########################
def scrape_proxies():
    print("[Scraper] Scraping proxy list...")
    url = 'https://free-proxy-list.net/'
    try:
        res = requests.get(url)
        soup = BeautifulSoup(res.text, 'html.parser')
        for row in soup.select("table tbody tr"):
            cols = row.find_all('td')
            ip = cols[0].text
            port = cols[1].text
            https = cols[6].text == 'yes'
            proxy_list.append(f"http{'s' if https else ''}://{ip}:{port}")
        print(f"[Scraper] Found {len(proxy_list)} proxies")
    except Exception as e:
        print(f"[Scraper] Error: {e}")

#########################
# 4. Proxy Rotator
#########################
def get_with_proxy(url):
    if not proxy_list:
        print("[Rotator] Proxy list is empty.")
        return
    proxy = random.choice(proxy_list)
    print(f"[Rotator] Using proxy: {proxy}")
    try:
        response = requests.get(url, proxies={"http": proxy, "https": proxy}, timeout=5)
        return response.text
    except Exception as e:
        return f"[Rotator] Failed: {e}"

#########################
# Main Runner
#########################
if __name__ == '__main__':
    scrape_proxies()
    threading.Thread(target=run_local_proxy, daemon=True).start()
    print("[Flask] Web proxy available at http://localhost:5000")
    app.run(port=5000)
